<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Lichang Chen</title>
  
  <meta name="author" content="Lichang Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lichang Chen</name>
              </p>
              <!-- bio -->
              <p> I am an AI research scientist at Meta Superintelligence Lab, working in a joint exploration team between MSL and TBD labs. I study agents for automating AI researchers and self-imporvement RL, trying to find the path towards AGI. I did my PhD at the <a href="https://www.cs.umd.edu/">Computer Science Department</a>, <a href="https://umd.edu/">University of Maryland, College Park</a>. I obtained my bachelor's degree from <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>. I can be reached at {bob}{my-last-name}@cs.umd.edu.
              </p>

              <p>
                My name in Chinese: ÈôàÂäõÁïÖ
              </p>
              <p style="text-align:center">
                <!-- <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=XesgJyUAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/LichangChen2">Twitter</a> &nbsp/&nbsp
		<a href="https://www.linkedin.com/in/lichang-chen-b7a506173/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/Lichang-Chen/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Lichang-Portrait.jpeg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/Lichang-Portrait.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>
              <p>
                My research interests are in RL and agentic systems,
                especially in self-evolving coding agent and how we can automate the realistic workflows such as foundational model research, Machine Learning, and Data Analysis.
                <!-- Representative papers are <span class="highlight">highlighted</span>  -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Working Experience</heading>
              <p>
                <strong style="font-size: 15px;">AI Research Scientist@Meta SuperIntelligence</strong>, 2025 - Present, Agents for automating AI researchers.
              </p>
              </td>
            </tr>
            </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Internship Experience</heading>
              <p>
                <strong style="font-size: 15px;">Intern@Google Deepmind</strong>, Meta RL for reasoning & Thinking to Learn.
                <br>
                <strong style="font-size: 15px;">Intern@Google Deepmind</strong>, The evaluation/alignment of Omni-Modality Language Models.
                <br>
                <strong style="font-size: 15px;">Intern@Google Research & Cloud AI Research</strong> The self-improvement of multimodal LLMs.
                <br>                              
                <strong style="font-size: 15px;">Intern@NVIDIA ADLR team</strong>, Mitigate hackings in RLHF/More Robust Reward Models
              </p>
              </td>
            </tr>
            </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/samsung.png" alt="PontTuset" width="140" style="border-style: none">
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nvidia-logo.webp" alt="Figure 2" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/google.png" alt="Figure 3" width="80" style="border-style: none">
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/deepmind.png" alt="Figure 4" width="100" style="border-style: none">
            </td>
          </tr>
              
            </p>
          </td>
        </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Publications</heading>
          </td>
        </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/omnify.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2410.12219">
                <papertitle>OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities</papertitle>
              </a>
              <br>
              <strong>Lichang Chen</strong>, Hexiang Hu, Pranav Shyam, Ming-Hsuan Yang, Boqing Gong, et al.
              <br>
              <em>Work Done@Google Deepmind</em>, <em>ICLR</em> 2025
              <br>
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/affirmative.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2409.11704">
                <papertitle>From Lists to Emojis: How Format Bias Affects Model Alignment</papertitle>
              </a>
              <br>
              <strong>Lichang Chen*</strong>, Xuanchang Zhang*, Wei Xiong*, Tianyi Zhou, Heng Huang, Tong Zhang.
              <br>
              <em>ACL</em>, 2025
              <br>
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rrm-image.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2409.13156">
                <papertitle>RRM: Robust Reward Model Training Mitigates Reward Hacking </papertitle>
              </a>
              <br>
              Tianqi Liu, Wei Xiong, Jie Ren, <strong>Lichang Chen</strong>, Tianhe Yu, Mohammad Saleh, et al.
              <br>
              <em>Work Done@Google Deepmind</em>, <em>ICLR</em> 2025
              <br>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ODIN.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2402.07319">
                <papertitle>ODIN: Disentangled Reward Mitigates Hacking in RLHF </papertitle>
              </a>
              <br>
               <strong>Lichang Chen*</strong>, Chen Zhu*, Davit Soselio, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro
              <br>
              <em>ICML</em>, 2024. 
              <br>
              <p></p>
            </td>
          </tr>     
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/instructzero.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2306.03082">
                <papertitle>InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models</papertitle>
              </a>
              <br>
              <strong>Lichang Chen*</strong>, Jiuhai Chen*, Tom Goldstein, Heng Huang, Tianyi Zhou
              <br>
              <em>ICML</em>, 2024
              <br>
              <p></p>
            </td>
          </tr>
	 <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/alpagasus.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2307.08701">
                <papertitle>AlpaGasus: Training a Better Alpaca with Fewer Data </papertitle>
              </a>
              <br>
              <strong>Lichang Chen*</strong>, Shiyang Li*, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, Hongxia Jin
              <br>
              <em>ICLR</em>, 2024
              <br>
              <p></p>
            </td>
          </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/backdoor.jpg" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2307.16888">
          <papertitle> Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection
          </papertitle>
        </a>
        <br>
        Jun Yan, Vikas Yadav, Shiyang Li, <strong>Lichang Chen</strong>, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin
        <br>
        <em> NAACL </em>, 2024
        <br>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/hallusionbench.jpg" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2310.14566.pdf">
          <papertitle>HallusionBench: an image-context reasoning benchmark challenging for multi-modality models </papertitle>
        </a>
        <br>
        Fuxiao Liu, Tianrui Guan, Zongxia Li,  <strong>Lichang Chen</strong>, et al.
        <br>
        <em>CVPR</em>, 2024
        <br>
        <p></p>
      </td>
    </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/demo-selection.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2303.08119">
                <papertitle>How Many Demonstrations Do You Need for In-context Learning?</papertitle>
              </a>
              <br>
              Jiuhai Chen, <strong>Lichang Chen</strong>, Chen Zhu, Tianyi Zhou
              <br>
              <em>EMNLP</em>, 2023
              <br>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ptp.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2305.02423.pdf">
                <papertitle>PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer</papertitle>
              </a>
              <br>
              <strong>Lichang Chen</strong>, Jiuhai Chen, Heng Huang, Minhao Cheng
              <br>
              <em>EMNLP</em>, 2023
              <br>
              <p></p>
            </td>
          </tr>
          

          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/backdoor.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2307.16888">
                <papertitle> Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection
                </papertitle>
              </a>
              <br>
              Jun Yan, Vikas Yadav, Shiyang Li, <strong>Lichang Chen</strong>, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin
              <br>
              <em> NAACL </em>, 2024
              <br>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hallusionbench.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2310.14566.pdf">
                <papertitle>HallusionBench: an image-context reasoning benchmark challenging for multi-modality models </papertitle>
              </a>
              <br>
              Fuxiao Liu, Tianrui Guan, Zongxia Li,  <strong>Lichang Chen</strong>, et al.
              <br>
              <em>CVPR</em>, 2024
              <br>
              <p></p>
            </td>
          </tr> -->
					
        </tbody></table>

				<!-- Misc -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table> -->

        <!-- <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
